{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "from scipy.spatial.distance import euclidean,mahalanobis\n",
    "from dtw import dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile = \"..\\\\data\\\\live-hip-hop-loop-81bpm-131102.mp3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Audio inside Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "rate must be specified when data is a numpy array or list of audio samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\frede\\florian-frey\\AudioSimilarity\\src\\librosa_test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/frede/florian-frey/AudioSimilarity/src/librosa_test.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m IPython\u001b[39m.\u001b[39;49mdisplay\u001b[39m.\u001b[39;49mAudio(audiofile)\n",
      "File \u001b[1;32mc:\\Users\\frede\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\lib\\display.py:129\u001b[0m, in \u001b[0;36mAudio.__init__\u001b[1;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m rate \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mrate must be specified when data is a numpy array or list of audio samples.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m Audio\u001b[39m.\u001b[39m_make_wav(data, rate, normalize)\n",
      "\u001b[1;31mValueError\u001b[0m: rate must be specified when data is a numpy array or list of audio samples."
     ]
    }
   ],
   "source": [
    "IPython.display.Audio(audiofile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spectrogram\n",
    "\n",
    "**Sampling**\n",
    "\n",
    "Sound is a continuous wave. We can digitise sound by breaking the continuous wave into discrete signals. This process is called sampling. Sampling converts a sound wave into a sequence of samples or a discrete-time signal.\n",
    "\n",
    "The load functions loads the audio file and converts it into an array of values which represent the amplitude if a sample at a given point of time.\n",
    "\n",
    "**Sampling Rate**\n",
    "\n",
    "The sampling rate is the number of samples per second. Hz or Hertz is the unit of the sampling rate. 20 kHz is the audible range for human beings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sr = librosa.load(audiofile)#, sr=22050, mono=True, offset=0.0, duration=50, res_type='kaiser_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "librosa.display.waveshow(data,sr=sr, max_points=5000, axis='time', transpose=False)\n",
    "# plt.xlim(0,10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## librosa beat extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile = librosa.example('nutcracker')\n",
    "\n",
    "y, sr = librosa.load(audiofile)\n",
    "\n",
    "# Run the default beat tracker\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "print('Estimated tempo: {:.2f} beats per minute'.format(tempo))\n",
    "\n",
    "# Convert the frame indices of beat events into timestamps\n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_len = len(y) / sr\n",
    "timestamps = np.linspace(0, audio_len, num=len(y))\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(timestamps, y)\n",
    "plt.ylabel('Signal Value')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.xlim(20, 30)\n",
    "plt.vlines([beat_times], ymin=-0.5, ymax=0.5, linestyles=\"dashed\", colors=\"black\", linewidth=0.5)\n",
    "plt.title(\"Beat Tracker Example: Nutcracker\")\n",
    "# plt.savefig(\"beat_extraction_sample.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/d4r3topk/comparing-audio-files-python/blob/master/mfcc.py\n",
    "\n",
    "#Loading audio files\n",
    "y1, sr1 = librosa.load('..\\\\data\\\\smooth-ac-guitar-loop-93bpm-137706.mp3') \n",
    "y2, sr2 = librosa.load('..\\\\data\\\\acoustic-guitar-loop-f-91bpm-132687.mp3') \n",
    "\n",
    "#Showing multiple plots using subplot\n",
    "plt.subplot(1, 2, 1) \n",
    "mfcc1 = librosa.feature.mfcc(y=y1,sr=sr1)   #Computing MFCC values\n",
    "librosa.display.specshow(mfcc1)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "mfcc2 = librosa.feature.mfcc(y=y2, sr=sr2)\n",
    "librosa.display.specshow(mfcc2)\n",
    "\n",
    "dist, _, cost, path = dtw(mfcc1.T, mfcc2.T, dist=euclidean)\n",
    "print(\"The normalized distance between the two : \",dist)   # 0 for similar audios \n",
    "\n",
    "plt.imshow(cost.T, origin='lower', cmap=plt.get_cmap('gray'), interpolation='nearest')\n",
    "plt.plot(path[0], path[1], 'w')   #creating plot for DTW\n",
    "\n",
    "plt.show()  #To display the plots graphically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
